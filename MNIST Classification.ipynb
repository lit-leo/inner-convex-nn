{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "%matplotlib inline\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the dataset.\n",
    "digits, targets = load_digits(return_X_y=True)\n",
    "digits = digits.astype(np.float32) / 255\n",
    "\n",
    "digits_train, digits_test, targets_train, targets_test = train_test_split(digits, targets, random_state=0)\n",
    "\n",
    "train_size = digits_train.shape[0]\n",
    "\n",
    "input_size = 8*8\n",
    "classes_n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTData(torch_data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(MNISTData, self).__init__()\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx],self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = MNISTData(digits_train, targets_train) \n",
    "val_dset = MNISTData(digits_test, targets_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonconvex fullyconnected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        self.fc0 = nn.Linear(64, 40)\n",
    "        self.fc1 = nn.Linear(40, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        h1 = F.relu(self.fc0(input_))\n",
    "        h2 = F.relu(self.fc1(h1))\n",
    "        h3 = self.fc2(h2)\n",
    "        return h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = FCN()  \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, save_dir=None):\n",
    "    net.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        loss = []\n",
    "        for X, y in train_loader:\n",
    "            nn_outputs = net(X)\n",
    "            loss1 = criterion(nn_outputs, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss1.backward()\n",
    "            loss.append(loss1.item())\n",
    "            optimizer.step()\n",
    "        net.eval()\n",
    "        val_loss = []\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            nn_outputs = net(X)\n",
    "            val_loss1 = criterion(nn_outputs,y)\n",
    "            val_loss.append(val_loss1.item())\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,1)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, np.mean(loss), np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 || Loss:  Train 1.4704 | Validation 1.4501\n",
      "Epoch 20/200 || Loss:  Train 0.8151 | Validation 0.8570\n",
      "Epoch 30/200 || Loss:  Train 0.5679 | Validation 0.6149\n",
      "Epoch 40/200 || Loss:  Train 0.4585 | Validation 0.4937\n",
      "Epoch 50/200 || Loss:  Train 0.3886 | Validation 0.4285\n",
      "Epoch 60/200 || Loss:  Train 0.3411 | Validation 0.3846\n",
      "Epoch 70/200 || Loss:  Train 0.2992 | Validation 0.3491\n",
      "Epoch 80/200 || Loss:  Train 0.2614 | Validation 0.3102\n",
      "Epoch 90/200 || Loss:  Train 0.2327 | Validation 0.2889\n",
      "Epoch 100/200 || Loss:  Train 0.2098 | Validation 0.2849\n",
      "Epoch 110/200 || Loss:  Train 0.1893 | Validation 0.2655\n",
      "Epoch 120/200 || Loss:  Train 0.1747 | Validation 0.2528\n",
      "Epoch 130/200 || Loss:  Train 0.1592 | Validation 0.2422\n",
      "Epoch 140/200 || Loss:  Train 0.1495 | Validation 0.2362\n",
      "Epoch 150/200 || Loss:  Train 0.1381 | Validation 0.2313\n",
      "Epoch 160/200 || Loss:  Train 0.1250 | Validation 0.2329\n",
      "Epoch 170/200 || Loss:  Train 0.1172 | Validation 0.2208\n",
      "Epoch 180/200 || Loss:  Train 0.1094 | Validation 0.2294\n",
      "Epoch 190/200 || Loss:  Train 0.1007 | Validation 0.2198\n",
      "Epoch 200/200 || Loss:  Train 0.0934 | Validation 0.2180\n"
     ]
    }
   ],
   "source": [
    "train(200, net, criterion, optimizer, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch_data.DataLoader(val_dset,batch_size = len(val_dset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9422222222222222\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "for X,y in test_loader:\n",
    "    X = X.to(device)\n",
    "    nn_outputs = net(X).detach().numpy().argmax(axis = 1)\n",
    "    print(accuracy_score(nn_outputs,y.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ICNN, self).__init__()\n",
    "        self.Wy0 = nn.Linear(64, 40)\n",
    "        self.Wy1 = nn.Linear(64, 20)\n",
    "        self.Wy2 = nn.Linear(64, 10)\n",
    "        self.Wz1 = nn.Linear(40,20, bias = False)\n",
    "        self.Wz2 = nn.Linear(20,10, bias = False)\n",
    "        self.Wz3 = nn.Linear(10,10)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        h1 = F.relu(self.Wy0(input_))\n",
    "        h2 = F.relu(self.Wz1(h1) + self.Wy1(input_))\n",
    "        h3 = F.relu(self.Wz2(h2) + self.Wy2(input_))\n",
    "        h4 = self.Wz3(h3)\n",
    "        return h4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = ICNN()  \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, save_dir=None):\n",
    "    net.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        loss = []\n",
    "        for X, y in train_loader:\n",
    "            nn_outputs = net(X)\n",
    "            loss1 = criterion(nn_outputs, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss1.backward()\n",
    "            loss.append(loss1.item())\n",
    "            optimizer.step()\n",
    "            net.Wz1.weight.data = F.relu(net.Wz1.weight.data)\n",
    "            net.Wz2.weight.data = F.relu(net.Wz2.weight.data)\n",
    "        net.eval()\n",
    "        val_loss = []\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            nn_outputs = net(X)\n",
    "            val_loss1 = criterion(nn_outputs,y)\n",
    "            val_loss.append(val_loss1.item())\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,1)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, np.mean(loss), np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 || Loss:  Train 1.7722 | Validation 1.7767\n",
      "Epoch 20/200 || Loss:  Train 0.8505 | Validation 0.9000\n",
      "Epoch 30/200 || Loss:  Train 0.4824 | Validation 0.5488\n",
      "Epoch 40/200 || Loss:  Train 0.3310 | Validation 0.3996\n",
      "Epoch 50/200 || Loss:  Train 0.2409 | Validation 0.3215\n",
      "Epoch 60/200 || Loss:  Train 0.1783 | Validation 0.2697\n",
      "Epoch 70/200 || Loss:  Train 0.1400 | Validation 0.2466\n",
      "Epoch 80/200 || Loss:  Train 0.1103 | Validation 0.2328\n",
      "Epoch 90/200 || Loss:  Train 0.0886 | Validation 0.2321\n",
      "Epoch 100/200 || Loss:  Train 0.0715 | Validation 0.2200\n",
      "Epoch 110/200 || Loss:  Train 0.0581 | Validation 0.2257\n",
      "Epoch 120/200 || Loss:  Train 0.0465 | Validation 0.2108\n",
      "Epoch 130/200 || Loss:  Train 0.0364 | Validation 0.2171\n",
      "Epoch 140/200 || Loss:  Train 0.0306 | Validation 0.2139\n",
      "Epoch 150/200 || Loss:  Train 0.0238 | Validation 0.2231\n",
      "Epoch 160/200 || Loss:  Train 0.0201 | Validation 0.2156\n",
      "Epoch 170/200 || Loss:  Train 0.0162 | Validation 0.2193\n",
      "Epoch 180/200 || Loss:  Train 0.0120 | Validation 0.2163\n",
      "Epoch 190/200 || Loss:  Train 0.0098 | Validation 0.2118\n",
      "Epoch 200/200 || Loss:  Train 0.0078 | Validation 0.2203\n"
     ]
    }
   ],
   "source": [
    "train(200, net, criterion, optimizer, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch_data.DataLoader(val_dset,batch_size = len(val_dset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9511111111111111\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "for X,y in test_loader:\n",
    "    X = X.to(device)\n",
    "    nn_outputs = net(X).detach().numpy().argmax(axis = 1)\n",
    "    print(accuracy_score(nn_outputs,y.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wz1 = net.Wz1.weight.data.detach().numpy()\n",
    "Wz2 = net.Wz2.weight.data.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Wz1: 0.28625\n",
      "Sparsity Wz2: 0.1\n"
     ]
    }
   ],
   "source": [
    "print('Sparsity Wz1: {}'.format(np.sum(Wz1==0)/(Wz1.shape[0]*Wz1.shape[1])))\n",
    "print('Sparsity Wz2: {}'.format(np.sum(Wz2==0)/(Wz2.shape[0]*Wz2.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = ICNN()  \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, save_dir=None):\n",
    "    net.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        loss = []\n",
    "        for X, y in train_loader:\n",
    "            nn_outputs = net(X)\n",
    "            loss1 = criterion(nn_outputs, y)\n",
    "            optimizer.zero_grad()\n",
    "            L1_reg = torch.tensor(0., requires_grad=True)\n",
    "            for name, param in net.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    L1_reg = L1_reg + torch.norm(param, 1)\n",
    "\n",
    "            loss1 += 1e-5 * L1_reg\n",
    "            loss1.backward()\n",
    "            loss.append(loss1.item())\n",
    "            optimizer.step()\n",
    "            net.Wz1.weight.data = F.relu(net.Wz1.weight.data)\n",
    "            net.Wz2.weight.data = F.relu(net.Wz2.weight.data)\n",
    "        net.eval()\n",
    "        val_loss = []\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            nn_outputs = net(X)\n",
    "            val_loss1 = criterion(nn_outputs,y)\n",
    "            val_loss.append(val_loss1.item())\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,1)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, np.mean(loss), np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 || Loss:  Train 1.7201 | Validation 1.6733\n",
      "Epoch 20/200 || Loss:  Train 0.8588 | Validation 0.8536\n",
      "Epoch 30/200 || Loss:  Train 0.5394 | Validation 0.5549\n",
      "Epoch 40/200 || Loss:  Train 0.3818 | Validation 0.4001\n",
      "Epoch 50/200 || Loss:  Train 0.2831 | Validation 0.3144\n",
      "Epoch 60/200 || Loss:  Train 0.2192 | Validation 0.2586\n",
      "Epoch 70/200 || Loss:  Train 0.1785 | Validation 0.2370\n",
      "Epoch 80/200 || Loss:  Train 0.1467 | Validation 0.2092\n",
      "Epoch 90/200 || Loss:  Train 0.1254 | Validation 0.1894\n",
      "Epoch 100/200 || Loss:  Train 0.1087 | Validation 0.1867\n",
      "Epoch 110/200 || Loss:  Train 0.0953 | Validation 0.1760\n",
      "Epoch 120/200 || Loss:  Train 0.0829 | Validation 0.1699\n",
      "Epoch 130/200 || Loss:  Train 0.0720 | Validation 0.1712\n",
      "Epoch 140/200 || Loss:  Train 0.0631 | Validation 0.1684\n",
      "Epoch 150/200 || Loss:  Train 0.0549 | Validation 0.1612\n",
      "Epoch 160/200 || Loss:  Train 0.0495 | Validation 0.1631\n",
      "Epoch 170/200 || Loss:  Train 0.0427 | Validation 0.1596\n",
      "Epoch 180/200 || Loss:  Train 0.0391 | Validation 0.1582\n",
      "Epoch 190/200 || Loss:  Train 0.0354 | Validation 0.1617\n",
      "Epoch 200/200 || Loss:  Train 0.0311 | Validation 0.1676\n"
     ]
    }
   ],
   "source": [
    "train(200, net, criterion, optimizer, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch_data.DataLoader(val_dset,batch_size = len(val_dset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "for X,y in test_loader:\n",
    "    X = X.to(device)\n",
    "    nn_outputs = net(X).detach().numpy().argmax(axis = 1)\n",
    "    print(accuracy_score(nn_outputs,y.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wz1 = net.Wz1.weight.data.detach().numpy()\n",
    "Wz2 = net.Wz2.weight.data.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity Wz1: 0.57\n",
      "Sparsity Wz2: 0.35\n"
     ]
    }
   ],
   "source": [
    "print('Sparsity Wz1: {}'.format(np.sum(Wz1==0)/(Wz1.shape[0]*Wz1.shape[1])))\n",
    "print('Sparsity Wz2: {}'.format(np.sum(Wz2==0)/(Wz2.shape[0]*Wz2.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
